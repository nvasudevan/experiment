#!/usr/bin/env python

# generates class specification with varying sizes of Rules and Alts
# Rule :: 4-6
# Alts :: 5-7
#
# first generate nonterminal tokens (so no of rules is no of nonterminals)
# ratio of nonterminal to terminal symbols is always 1:1

# Haskell data-types generated will of this form:
#
#data Prog = Prog Rule Rule Rule Rule Rule deriving (Typeable, Data, Show)
#data Rule = RuleAlts Alt Alt Alt Alt deriving (Typeable, Data, Show)
#data Alt = AltTokens [Symbol]  deriving (Typeable, Data, Show)
#data Symbol = NonTerm NT | Term T deriving (Typeable, Data, Show)
#data NT = A | B | C | D | E deriving (Typeable, Data, Show)
#data T = T1 | T2 | T3 | T4 | T5 deriving (Typeable, Data, Show)

import random, string, StringIO, random, getopt
import os, subprocess, sys
from sets import Set
  
def usage(msg=None):
    sys.stderr.write("genBoltz -d <grammar directory> -n <no of samples> -p <precision in float> -r <no of nonterminals> -t <short|long>\n\n")
    sys.exit(1)

def generateLex(terms_map,grammardir):
    lex_file = grammardir + "/" + "boltz.lex"
    f_lex = open(lex_file,"w")
    # first write the usual lines
    f_lex.write("%{" + "\n")
    f_lex.write('#include "yygrammar.h"' + "\n")
    f_lex.write("%}" + "\n")
    f_lex.write("%%" + "\n")
    f_lex.write('" "    { /* skip blank */ }\n')
    f_lex.write('\\n     { yypos++; /* adjust linenumber and skip newline */ }' + "\n")
    f_lex.write('\\r     { yypos++; /* adjust linenumber and skip newline */ }' + "\n")
    #
    f_lex.write('.      { yyerror("illegal token"); }' + "\n")
    # now write your stuff
    for key in terms_map.keys():
        #"delete"     { return DELETE; }
        f_lex.write('"' + terms_map[key] + '"     { return ' + key + "; }\n")
    f_lex.close()

def generateGrammars(no_samples, sing_prec, exe, nonterms, terms_map, sym_type, grammardir):
    p = subprocess.Popen([exe,str(no_samples),str(sing_prec)], stdout=subprocess.PIPE)
    out = p.communicate()
    out1 = out[0].replace("Prog (RuleAlts (AltTokens","\nCFG\n\nRule:")
    out2 = out1.replace("(RuleAlts (AltTokens","\n\nRule:")
    out3 = out2.replace("(AltTokens","|")
    out4 = out3.replace(")","")
    out5 = out4.replace("NonTerm","")
    out6 = out5.replace("Term","")
    #print "before: " , out6
#    if sym_type == "short":
#        for key in terms_map.keys():
#            out6 = out6.replace(key,"'" + terms_map[key] + "'")
       
    out7 = out6.replace("]","")
    out8 = out7.replace("[","")
    out9 = out8.replace(",","")
#    print "--"
#    print out9
#    print "--"
    #buf = StringIO.StringIO(out9)
    cfg_list = out9.split("CFG")
#            for cfg in cfg_list[1:]:
#                print "XX: \n" , cfg
#            sys.exit(1)
    nonterms.append("root")
    count=1
    for cfg in cfg_list[1:]:
        buf = StringIO.StringIO(cfg)
        rules=[]
        while 1:
            line = buf.readline()
            if not line:
                break
            if line.startswith("Rule:"):
                rules.append(line.split("Rule:")[1])
    

        random.shuffle(nonterms)
        random.shuffle(rules)
        rules_dict = dict(zip(nonterms,rules))        
        sys.stdout.write(".")
        sys.stdout.flush()
        cfg_file = grammardir + "/" + str(count) + ".acc"
        f_cfg = open(cfg_file,"w")
        root_rule = rules_dict.pop('root')
        f_cfg.write("%token " + ", ".join(terms_map.keys()) + ";\n\n")
        f_cfg.write("%nodefault\n\n")
        f_cfg.write("root: " + root_rule + ";\n")
        rules_keys = rules_dict.keys()
        for key in rules_keys:
            print " ** " , key, rules_dict[key]
            f_cfg.write(key + ": " + rules_dict.pop(key) + ";\n")
        f_cfg.close()
        count += 1

def uniqueSymbols(n_nonterms,n_terms):
    symbols =  Set()
    # we need one less nonterminal as we have start symbol - root
    while (len(symbols) < ((n_nonterms-1)+n_terms)):
        sym = ''.join(random.choice(string.uppercase) for x in range(random.randint(2,5)))
        symbols.add(sym)

    symlist = list(symbols)
    print symlist    
    return symlist[0:n_nonterms-1],symlist[n_nonterms-1:]
    

def genSymbols(sym_type,n_nonterms,n_terms):
    if sym_type == "short":
        # one of the nonterminal will be 'root'. so we need one less
        nonterms = random.sample(string.uppercase,n_nonterms-1)
        terms = random.sample(string.lowercase,n_terms)
    else:
        # nonterms and terms as words
        nonterms,terms = uniqueSymbols(n_nonterms,n_terms)  
        
    return nonterms,terms

def generateDataType(rule_size_range,alt_size_range,sym_type,no_samples,sing_prec,grammardir):
    for nRules in rule_size_range:
        for nAlts in alt_size_range:
            nonterms,terms = genSymbols(sym_type,nRules,nRules)            
            print nonterms,terms   
            terms_map = {('TK_' + i): i for i in terms}
            h_terms = terms_map.keys()
            print "--\n",terms_map
            header1 = "{-# LANGUAGE DeriveDataTypeable #-}"
            header2 = "module Cfg where"
            header3 = "import Data.Generics"
            prog_rule = "data Prog = Prog " + nRules * "Rule " + "deriving (Typeable, Data, Show)"
            rule_rule = "data Rule = RuleAlts " + nAlts * "Alt " + "deriving (Typeable, Data, Show)"
            alt_rule = "data Alt = AltTokens [Symbol]  deriving (Typeable, Data, Show)"
            symbol_rule = "data Symbol = NonTerm NonTerm | Term Term deriving (Typeable, Data, Show)"
            nt_rule = "data NonTerm = " + " | ".join(nonterms) + " deriving (Typeable, Data, Show)"
            t_rule = "data Term = " + " | ".join(h_terms) + " deriving (Typeable, Data, Show)"
            cfg = "%s \n%s \n%s \n\n%s \n%s \n%s \n%s \n%s \n%s\n" % (header1, header2, header3, prog_rule, rule_rule, alt_rule, symbol_rule, nt_rule, t_rule)
            print cfg
            cfg_file = "grammars/" + str(nRules) + "Rules_" + str(nAlts) + "Alts.hs"
            f_cfg = open(cfg_file,"w")
            f_cfg.write(cfg)
            f_cfg.close()
            
            # invoke the boltzmann for this grammar
            # make clean;make and then ./test/prog 1.0e-03 1
            r = subprocess.call(["cp",cfg_file,"test/Cfg.hs"])
            if r != 0:
                sys.stderr.msg("Copy failed\n")
                sys.exit(r)
                
            DEVNULL = open(os.devnull, 'wb')
            r = subprocess.call(["make","clean"],stdout=DEVNULL)
            
            if r != 0:
                sys.stderr.msg("Make clean failed\n")
                sys.exit(r)
                
            r = subprocess.call(["make"],stdout=DEVNULL)
            if r != 0:
                sys.stderr.msg("Make failed\n")
                sys.exit(r)       
                
            print "generating grammars..."
            generateGrammars(no_samples,sing_prec,"./test/prog",nonterms,terms_map,sym_type,grammardir)
            print "\ngenerating lex..."
            generateLex(terms_map,grammardir)
            print ".. DONE .."
                

if __name__ == "__main__":
    no_samples = None
    sing_prec = None
    no_nt = None
    sym_type = None
    grammardir = None
    opts, args = getopt.getopt(sys.argv[1 : ], "hn:p:r:t:d:")
    for opt in opts:
        if opt[0] == "-n":
            no_samples=int(opt[1])
        elif opt[0] == "-p":
            sing_prec=float(opt[1])
        elif opt[0] == "-r":
            no_nt = int(opt[1])
        elif opt[0] == "-t":
            sym_type = opt[1] # short or long   
        elif opt[0] == "-d":
            grammardir = opt[1]                 
        elif opt[0] == "-h":
            usage()
    
    if (no_samples == None) or (sing_prec == None) or (no_nt == None) or (sym_type == None) or (grammardir == None):
        usage()
    
    # we create the directory here.    
    if not os.path.exists(grammardir):
        os.makedirs(grammardir)
                    
#    print "no_samples, precision: %s, %s"  % (no_samples,sing_prec)
    rule_size_range=[no_nt]
    alt_size_range=[5]
    generateDataType(rule_size_range,alt_size_range,sym_type,no_samples,sing_prec,grammardir)

                
                
    
            
    
